
# Define the FGSM attack function
fgsm_attack <- function(image, model, epsilon) {
  # Convert the image to an array and add an extra dimension (batch size of 1)
  image_array <- array_reshape(image, c(1, dim(image)))
  
  # Normalize the image data by dividing by 255
  image_array <- image_array / 255
  
  
  # The gradient is the rate of change of the loss function with respect to the input image
  # Create a TensorFlow gradient tape to record operations for automatic differentiation
  # Records Operations for Automatic Differentiation
  # Automatic Differentiation computes the gradient/derivative of a function with respect to its input images
  tape <- tf$GradientTape()
  
  # Wrap operations inside the gradient tape context
  with(tape, {
    # Set the image as a watched variable for gradient calculation
    tape$watch(image_array)
    
    # Get the model prediction for the image
    prediction <- model %>% predict(image_array)
    
    # Calculate the loss using the true label
    # CategoricalCrossentropy calculates the difference between the predicted probability distribution and the true probability distribution. 
    # Lower cross-entropy loss means better predictions.
    loss <- tf$keras$losses$CategoricalCrossentropy()(image_array, prediction)
  })
  
  # Get the gradient of the loss with respect to the input image
  # The gradient is the rate of change of the loss function with respect to the input image
  gradient <- tape$gradient(loss, image_array)
  
  # Calculate the sign of the gradient (important because it gives the "direction" of how the pixels are modified in order to fool the classifier)
  signed_grad <- tf$sign(gradient)
  
  # Generate the adversarial image using the signed gradient and epsilon
  # Epsilon is a small scalar value that controls the magnitude of the changes added to the input image
  adversarial_image <- image_array + epsilon * signed_grad
  
  # Clip the adversarial image to keep the pixel values in the valid range (0 to 1)
  adversarial_image <- tf$clip_by_value(adversarial_image, 0, 1)
  
  # Convert back to the original pixel value range (0 to 255)
  return(adversarial_image * 255)  
}

#Now test this FGSM algorithm meant to fool classifier on main algorithm 

# Load the pre-trained image classification model
model <- load_model_tf("./dandelion_model")

# Load and preprocess the input image
target_size <- c(224, 224)
input_image <- image_load("./grass/modified_grass.jpg", target_size = target_size)
input_array <- image_to_array(input_image)

# Define the epsilon value for the FGSM attack
epsilon <- 0.01

# Perform the FGSM attack to generate an adversarial image
adversarial_array <- fgsm_attack(input_array, model, epsilon)

# Save the adversarial image
writeJPEG(adversarial_array, "./grass/modified_grass.jpg")

