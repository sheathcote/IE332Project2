library(mxnet)
library(opencv)
library(magrittr)
library(jpeg)

model <- mx.model.load('model-symbol.json', 'model-0000.params', ctx = mx.cpu(), type = "mxnet")

img <- mx.nd.array(cv2_imread("image.jpg") %>% cv2.cvtColor(cv2.COLOR_BGR2RGB))

max_iter <- 100

epsilon <- 0.3

min_dist <- 0.1

#number of classes
num_classes <- 2

get_gradient <- function(img, label){
  #gradient accumulators zero
  grad <- mx.nd.zeros(dim = dim(img), ctx = mx.cpu())
  label_onehot <- mx.nd.one_hot(mx.nd.array(label), num_classes)
  # Forward pass
  with(mx.autograd.record(), {
    output <- model(img)
    loss <- mx.nd.sum(label_onehot * output)
  })
  # Backward pass
  loss.backward()
  grad <- img$grad
  return(grad)
}

deep_fool <- function(img, label, max_iter, epsilon, min_dist){
  
  x <- img %>% mx.nd.copy()
  
  label <- label
  
  iter <- 0
  
  grad_orig <- get_gradient(x, label)
  

  y <- model(x)$as.numeric()
  
 
  pert_min <- Inf
  
 
  while(iter < max_iter && pert_min > min_dist)
  {
   
  
    pert_min_curr <- Inf
    
   
    for(k in setdiff(seq_len(num_classes), y))
    {
      # Calculating the gradient of the image with respect to the current class label
      grad_k <- get_gradient(x, k)
      
      # Calculating the difference between the gradient of the current class label and the gradient of the predicted class label
      w_k <- grad_k - grad_orig
      
      # Calculating the L2 norm of the difference
      norm_w_k <- mx.nd.norm(w_k, ord = 2) %>% as.numeric()
      
      # Calculating the perturbation vector
      pert_k <- (epsilon * w_k / norm_w_k)
      
      # Calculating the L2 norm of the perturbation vector
      pert_norm_k <- mx.nd.norm(pert_k, ord = 2) %>% as.numeric()
      
      
      if(pert_norm_k < pert_min_curr)
      {
        pert_min_curr <- pert_norm_k
        pert_k_min <- pert_k
      }
    }
  
    x <- x + pert_k_min
    # Clipping the image to the valid range [0, 1]
    }
   
