\documentclass[11pt]{article}

%  USE PACKAGES  ---------------------- 
\usepackage[margin=0.75in,vmargin=1in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{hyperref,color}
\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{4}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\HREF}[2]{\href{#1}{#2}}
\usepackage{textcomp}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
% columns=flexible,
upquote=true,
breaklines=true,
showstringspaces=false
}

\renewcommand\paragraph{{\normalfont\normalsize}}
%  -------------------------------------------- 
%https://www.overleaf.com/project/63cab2b14d247bee612dcc29https://www.overleaf.com/project/63cab2b14d247bee612dcc29
%  HEADER AND FOOTER (DO NOT EDIT) ----------------------
\newcommand{\problemnumber}{0}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\textbf{\sectionheader}}
\newcommand{\newquestion}[1]{
\clearpage % page break and flush floats
\renewcommand{\problemnumber}{#1} % set problem number for header
\phantom{}  % Put something on the page so it shows
}
\fancyfoot[L]{IE 332}
\fancyfoot[C]{Assignment submission}
\fancyfoot[R]{Page \thepage}
\renewcommand{\footrulewidth}{0.4pt}

%  --------------------------------------------


%  COVER SHEET (FILL IN THE TABLE AS INSTRUCTED IN THE ASSIGNMENT) ----------------------
\newcommand{\addcoversheet}{
\clearpage
\thispagestyle{empty}
\vspace*{0.5in}

\begin{center}
\Huge{{\bf IE332 Project 2}} % <-- replace with correct assignment #

Due: April 28th, 11:59pm EST % <-- replace with correct due date and time
\end{center}

\vspace{0.3in}

\noindent We have {\bf read and understood the assignment instructions}. We certify that the submitted work does not violate any academic misconduct rules, and that it is solely our own work. By listing our names below we acknowledge that any misconduct will result in appropriate consequences. 

\vspace{0.2in}

\noindent {\em ``As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do.
Accountable together -- we are Purdue.''}

\vspace{0.3in}

\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{c|ccccc|c|c}
      Student & Algorithms & IDK & What should & Go Here & Report & Overall & DIFF\\
      \hline
      Alec F & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Dhruv G & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Glen T & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Sami H & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      Varun R & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
      
      \hline
      St Dev & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{tabular}
  \end{center}
\end{table}

\vspace{0.2in}

\noindent Date: \today.
}
%  -----------------------------------------

%  TODO LIST (COMPLETE THE FULL CHECKLIST - USE AS EXAMPLE THE FIRST CHECKED BOXES!) ----------------------
\newcommand{\addtodo}{
\clearpage
\thispagestyle{empty}

\section*{Read Carefully. Important!}

\noindent By electronically uploading this assignment to Brightspace you acknowledge these statements and accept any repercussions if in any violation of ANY Purdue Academic Misconduct policies. You must upload your homework on time for it to be graded. No late assignments will be accepted. {\bf Only the last uploaded version of your assignment before the due date will be graded}.

\vspace{0.2in}

\noindent {\bf NOTE:} You should aim to submit no later than 30 minutes before the deadline, as there could be last minute network traffic that would cause your assignment to be late, resulting in a grade of zero. 

\vspace{0.2in}

\noindent When submitting your assignment it is assumed that every student considers the below checklist, as there are grading consequences otherwise (e.g., not submitting a cover sheet is an automatic grade of ZERO).

\begin{todolist}

    \item[\done] Your solutions were prepared using the \LaTeX template provided in Brightspace. 
    \item[\done] Your submission has a cover sheet as its first page and this checklist as its second page, according to the template provided.
	 \item[\done] All of your solutions (program code, etc.) are included in the submission as requested. % Check this checkbox and the following ones if satisfied <---
    \item[\done] You have not included any screen shots, photos, etc. (plots should be intermediately saved as .png files and then added into your .tex file). % <---
	 \item[\done] All math notation and algorithms (algorithmic environment) are created using appropriate \LaTeX code (no pictures, handwritten solutions, etc.). % <---
    \item[\done] The .pdf is submitted as an individual file and not in a {\tt .zip}.
    \item[\done] You kept the \LaTeX source code in your files until this assignment is graded, in case you are required to show proof of creating your assignment using \LaTeX.  % <---
    \item[\done] If submitting with a partner, your partner is added in the submission section in Gradescope after you upload your file. % <---
    \item[\done] You have correctly matched each question to its page \# in the .pdf submission in the Gradescope section (after you uploaded your file).
    \item[\done] Watch videos on creating pseudocode if you need a refresher or quick reference to the idea. These are good starter videos:    % <---
    
     \HREF{https://www.youtube.com/watch?v=4jLO0vXPktU}{www.youtube.com/watch?v=4jLO0vXPktU} 
    
    \HREF{https://www.youtube.com/watch?v=yGvfltxHKUU}{www.youtube.com/watch?v=yGvfltxHKUU}
\end{todolist}
}

%% LaTeX
% Für alle, die die Schönheit von Wissenschaft anderen zeigen wollen
% For anyone who wants to show the beauty of science to others

%  -----------------------------------------


\begin{document}


\addcoversheet
\addtodo

\newpage
\tableofcontents
% BEGIN YOUR ASSIGNMENT HERE:

\newpage
\section{Main Text}
\subsection{Introduction}
\paragraph{As the world moves toward more reliance on AI and robotics, it is important that we understand how adversarial attacks can be used to fool machine learning tools, such as image classifiers. Understanding and being able to stop adversarial attacks could have a large impact on the future, especially in situations such as autonomous driving, where the consequences of classifying a stop sign incorrectly could be death (Hui, 2020).}

\subsection{Jacobian-based Saliency Map Attack (JSMA)}
\paragraph{The Jacobian-based Saliency Map Attack is a greedy algorithm that changes one pixel at a time over many iterations until the modified picture can fool the image classifier model into mislabeling it. This algorithm takes in an image, a model used to classify the images, a target label, and a maximum number of changes. It then iteratively calls a JSMA function that uses a saliency map to decide what pixel to change next. A saliency map is created by computing the change in the logit score of the target label for each pixel. Then, the pixel with the largest change to the logit score is chosen and changed. This is repeated until the image is able to fool the image classifier or the maximum number of changes is reached. (Dahal, 2019)}
\paragraph{This type of attack is a good fit for our project because we have a limited budget of pixels, so an algorithm that changes one pixel at a time would be easy to stop when it reaches the pixel budget. Furthermore, since this function takes in a maximum number of changes variable, we can easily modify this variable to be the chosen pixel budget. In adition to being easy to limit the number of changes made by this algorithm, since this algorithm focuses on changing pixels that have the largest effect on the logit score of the target label, the algorithm should still be somewhat effective when the pixel allowance is small.}

\subsection{Fast Gradient Signed Method Attack}
\paragraph{ 
The FGSM algorithm begins by loading the necessary libraries, then loading the model using load_model_tf, and then a custom function called random is used to randomly change specific pixel values. 
The main FGSM perturbation function is what is mainly used to make this algorithm attack work. Perturbation is another word for change which is a term used for the FGSM attack. 
It takes four arguments: image, model, epsilon, and target_label. The image is the model's input, the model is the Brightspace model/classifier, epsilon is a scalar value created which controls the magnitude or level of the perturbation, and target_label is the desired output label for the attack. 
The function then calculates the gradient (rate of change/derivative) of the loss with respect to the input image using the TensorFlow GradientTape (a tool used to calculate gradients for the inputted images). 
Then, the perturbation is calculated by multiplying the signed gradient with the epsilon value and then putting that into the original images. 
These original images are provided in the algorithm in two different sections: grass and dandelions. For each section, the images are loaded and then converted into arrays so that they are in the right format to be affected by the FGSM attack. 
Then the perturbed (or changed) image is clipped so that the pixel values are in a valid range of 0 to 1 so that they can be properly processed. 
Finally, the prediction created by the FGSM algorithm previously are made using the Brightspace model and the predictions results are printed or outputted. 
Ultimately, the FGSM attack fools the model by introducing small perturbations (changes) to the input images, making the model misclassify them while keeping the changes imperceptible to humans. 
For my FGSM attack, it fooled the model by having it classify both the grass and dandelion images as grass ([,2] was close to 1 while [,1] was 0 for all the images and [,2] refers to grass probability while [,1] refers to dandelion probability). 

\subsection{PGD Attack}
\paragraph{The Projected Gradient Descent attack is an optimization algorithm that uses white-box attacks in an attempt to perturb images enough so that they can fool an image 
classifier. When researching different types of algorithms to try and work on to fool the pre trained classifier, there were 4 main types of adversarial attacks to consider: 
poisoning, evasion, extraction, and interference. [1] The most simple, yet feasible option seemed to be an evasion attack because it involves the use of a pre trained model, 
and it only modifies the input to the model, not the actual training data, unlike the other three options. When looking at the different types of evasion attacks, two stood out: 
FGSM or Fast Gradient Signed Method and PGD or Projected Gradient Descent. Both involve using a gradient to perturb an image a predetermined amount to add noise and try to fool the classifier. 
The most simple way of explaining how a gradient based adversarial atack works is that the algorithm computes the gradient of the model's output with respect to its input, and uses 
it to determine how to perturb the input. FGSM computes the gradient of loss function, a measure of how well the algorithm is performing, with respect to the input and then adds 
perturbation in the direction of the sign of the gradient. PGD applies FGSM iteratively to the input and projects the perturbed input onto the feasible set of inputs, for example 
those within a specificed distance of the original input. [2] Since PGD is an interative process, it looks for the perturbation that makes the loss function maximized while 
ensuring the perturbation is less than the specified input epsilon but that will be discusses in the appendicies.}

\subsection{Deep Fool}
\paragraph{Deep Fool is an algorithm with to machine learning models that are designed to cause the model to make incorrect predictions. One of the key advantages of Deep Fool is that it can quickly generate highly effective adversarial examples using a minor amount of confusion. This is because Deep Fool uses linear approximation on the decision boundary of the model, permitting it to find the smallest possible perturbations, causing the model to change its prediction. Additionally, Deep Fool is effective against a wide range of machine learning models, including both deep neural networks and more traditional models like support vector machines.

The algorithm works by computing the distance between a given input image and the decision boundary of the target model. At each iteration, the algorithm finds the direction in which the input can be most effectively perturbed to move it closer to the decision boundary, and then applies a small perturbation in that direction. The size of the perturbation is chosen to be as small as possible while still being enough to cause the model to change its prediction. This process is repeated until the input image is classified incorrectly by the model.

One of the reasons why Deep Fool is a good choice for generating adversarial examples is that it is relatively easy to implement. Because it uses a linear approximation of the decision boundary, it is often more efficient than other algorithms that rely on gradient information. This means that it can generate adversarial examples more quickly and with fewer computational resources. Finally, Deep Fool is able to generate highly effective adversarial examples that are often difficult for humans to distinguish from the original input. This makes it a powerful tool for testing the robustness of machine learning models and identifying potential vulnerabilities. Based on all this information we are using Deep Fool as it will be efficient in confusing the images.

\subsection{Other Algorithms Discussion}
\subsubsection{Spatial Transformation Attack}
\paragraph{The Spatial Transformation Attack that I attempted utilized affine transformations on the input images as opposed to the gradient-based changes of the FGSM. Then like the FGSM, it normalized the images to an appropriate pixel range (0 to 1 for pixel values as opposed to 0 to 255 for integers). Ultimately after a lot of editing and debugging, I got the attack to give an output but it wasn't in the format I wanted. While the attack's output did show the time taken for each image, it wouldn't show the probability rating for grass and dandelions for each image. I tried to hardcode some of the output syntax but that only made the end result messier. Ultimately, I decided that FGSM attack was simpler and more effective than this attack.}
\subsubsection{One Pixel Attack}
\paragraph{ The One Pixel Attack that I attempted as the name suggests change one pixel in the image to fool the model/classifier. Ultimately, I found this attack inefficient and unreliable, taking on significantly more runtime (250 milliseconds per image) all while giving messier output. Once I found out about FGSM and its use of an episilon based equation, I decided to stop using the One Pixel Attack}



}

\subsection{Conclusion}

\section{Appendix}

\subsection{Testing/Correctness/Verification}
\subsection{Runtime Complexity and Walltime}
\subsection{Performance}
\subsection{Algorithm Justification}
\subsection{Reasons to Choose FGSM}
\paragraph
{1. FGSM is a one-step method that generates adversarial examples by linearizing the loss function and maximizing it using the gradient sign. This basically means it operates from just one formula, making it both simpler and faster than alternatives (Deepfool, One Pixel, etc.) }
\newline
\paragraph{2. Everything about the magnitude and scale of the perturbations is through a simple epsilon scalar value, contributing to the aforementioned simplicity compared to the other options.
\subsubsection{FGSM Debugging}
\paragraph{Issue I faced: Incompatibility between the perturbation function and the input image. The perturbation function calculates the gradients for the input image with respect to the model's loss. These gradients sometimes produced perturbations that, when added to the original image, result in pixel values outside the valid range of [0, 1] thus creating an error. Clipping the perturbed image values to this valid range ensured compatibility with the input format expected by the model.}

\paragraph{Solution I chose: I used the k_clip function from the keras package to clip the pixel values of the perturbed image to the range [0, 1] after adding the perturbations.}

\paragraph{ Issue I faced: Reshaping arrays in the perturbation function. The input image needed to be reshaped into a 4-dimensional array with a batch dimension (1, height, width, channels) to be compatible with the model's input. The target_label also needed to be reshaped to match the model's output dimensions. I kept getting errors regarding incompatibility between the fgsm perturbation function and the input images' format. }

\paragraph{Solution I chose: Therefore, I ended up using the array_reshape function from the keras library to reshape the input image and target_label to the appropriate dimensions.}

\paragraph{Issue I faced: The error "non-numeric argument to binary operator" when trying to divide the image array by 255. Basically format incompatibility regarding dividing an image (non-numeric) by a numeric operator (255).} 

\paragraph{Solution I chose: To solve the issue, I ended up using the image_to_array function so that the input image turned array could be divided by 255. Dividing by 255 is necessary since it normalizes the pixel values to a 0 to 1 range which is much easier for the model to process as opposed to 1 to 255 integers.}

\paragraph{For deep fool there was one error:  could not find function "mx.nd.array". I figured out that I did not have the appropriate library downloaded for it to find the function. Once I installed the package opencv then the code overall was functioning except that I am not able to see the result. If the algorithm were to work then we would be able to see whether or not we fooled the compiler and it was still the same to the naked eye. Right now I have a set epsilon value of .3 as this worked on one of our other algorithms.  

\section{References}

\paragraph{Dahal, P. (2019, July 30). Adversarial attacks - breaking and defending neural networks. DeepNotes. Retrieved April 27, 2023, from https://deepnotes.io/adversarial-attack}

\paragraph{Hui, J. (2020, July 22). Adversarial attacks. Medium. Retrieved April 27, 2023, from https://jonathan-hui.medium.com/adversarial-attacks-b58318bb497b#:~:text=Jacobian%2Dbased%20Saliency%20Map%20Attack%20(JSMA)&amp;text=This%20is%20a%20greedy%20algorithm,label)%20for%20a%20saliency%20map.}

\paragraph{[1] Korolov, Maria. “Adversarial Machine Learning Explained: How Attackers Disrupt AI and ML Systems.” CSO Online, 28 June 2022, www.csoonline.com/article/3664748/adversarial-machine-learning-explained-how-attackers-disrupt-ai-and-ml-systems.html#:~:text=Types%20of%20adversarial%20machine%20learning,evasion%2C%20extraction%2C%20and%20inference. Accessed 27 Apr. 2023.}

\paragraph{[2] Knagg, Oscar. “Know Your Enemy - towards Data Science.” Medium, Towards Data Science, 6 Jan. 2019, towardsdatascience.com/know-your-enemy-7f7c5038bdf3. Accessed 27 Apr. 2023.}

‌
\end{document}
